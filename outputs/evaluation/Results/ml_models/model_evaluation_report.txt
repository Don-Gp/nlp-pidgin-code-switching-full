ENGLISH-PIDGIN CODE-SWITCHING MODEL EVALUATION REPORT
================================================================================

Report generated: 2025-03-19 13:19:28
Ground truth file: ground_truth.txt
Number of models evaluated: 37

MODEL RANKING BY ACCURACY
--------------------------------------------------------------------------------
Rank Model                         Accuracy  F1-Score  Precision Recall    
--------------------------------------------------------------------------------
1    test_ngram_2_random_forest    0.89      0.88      0.88      0.88      
2    test_ngram_3_random_forest    0.89      0.89      0.89      0.89      
3    test_ngram_4_random_forest    0.89      0.89      0.89      0.89      
4    test_ngram_5_random_forest    0.89      0.89      0.89      0.89      
5    test_ngram_6_random_forest    0.89      0.88      0.89      0.88      
6    test_ngram_6_logistic_regression0.87      0.87      0.87      0.87      
7    test_ngram_6_naive_bayes      0.85      0.85      0.85      0.85      
8    test_ngram_5_logistic_regression0.84      0.84      0.84      0.84      
9    test_ngram_5_naive_bayes      0.82      0.81      0.82      0.81      
10   test_ngram_4_logistic_regression0.80      0.79      0.80      0.79      
11   test_bilstm                   0.78      0.77      0.77      0.77      
12   test_ngram_1_random_forest    0.78      0.77      0.77      0.77      
13   test_ngram_4_naive_bayes      0.78      0.77      0.78      0.77      
14   test_ngram_3_logistic_regression0.76      0.75      0.75      0.75      
15   test_ngram_3_naive_bayes      0.74      0.73      0.73      0.73      
16   test_ngram_2_logistic_regression0.69      0.68      0.68      0.68      
17   test_ngram_2_naive_bayes      0.68      0.67      0.67      0.67      
18   test_ngram_1_logistic_regression0.61      0.60      0.60      0.61      
19   test_ngram_8_random_forest    0.61      0.54      0.61      0.57      
20   test_ngram_1_naive_bayes      0.60      0.59      0.59      0.59      
21   test_word_random_forest       0.52      0.48      0.62      0.57      
22   test_word_logistic_regression 0.44      0.32      0.62      0.51      
23   test_ngram_1_svm              0.43      0.30      0.22      0.50      
24   test_ngram_2_svm              0.43      0.30      0.22      0.50      
25   test_ngram_3_svm              0.43      0.30      0.57      0.50      
26   test_ngram_4_svm              0.43      0.30      0.22      0.50      
27   test_ngram_5_svm              0.43      0.30      0.22      0.50      
28   test_ngram_6_svm              0.43      0.30      0.22      0.50      
29   test_ngram_7_logistic_regression0.43      0.30      0.22      0.50      
30   test_ngram_7_naive_bayes      0.43      0.30      0.22      0.50      
31   test_ngram_7_random_forest    0.43      0.30      0.22      0.50      
32   test_ngram_7_svm              0.43      0.30      0.22      0.50      
33   test_ngram_8_logistic_regression0.43      0.31      0.61      0.50      
34   test_ngram_8_naive_bayes      0.43      0.30      0.61      0.50      
35   test_ngram_8_svm              0.43      0.30      0.59      0.50      
36   test_word_naive_bayes         0.43      0.31      0.70      0.50      
37   test_word_svm                 0.43      0.31      0.61      0.50      


BILSTM MODEL RESULTS
--------------------------------------------------------------------------------
Model                         Accuracy  F1-Score  English F1  Pidgin F1   
--------------------------------------------------------------------------------
test_bilstm                   0.78      0.77      0.81        0.73        


NGRAM MODEL RESULTS
--------------------------------------------------------------------------------
Model                         Accuracy  F1-Score  English F1  Pidgin F1   
--------------------------------------------------------------------------------
test_ngram_2_random_forest    0.89      0.88      0.90        0.87        
test_ngram_3_random_forest    0.89      0.89      0.91        0.87        
test_ngram_4_random_forest    0.89      0.89      0.90        0.87        
test_ngram_5_random_forest    0.89      0.89      0.91        0.87        
test_ngram_6_random_forest    0.89      0.88      0.90        0.86        
test_ngram_6_logistic_regression0.87      0.87      0.89        0.85        
test_ngram_6_naive_bayes      0.85      0.85      0.88        0.82        
test_ngram_5_logistic_regression0.84      0.84      0.86        0.82        
test_ngram_5_naive_bayes      0.82      0.81      0.85        0.78        
test_ngram_4_logistic_regression0.80      0.79      0.83        0.76        
test_ngram_1_random_forest    0.78      0.77      0.80        0.75        
test_ngram_4_naive_bayes      0.78      0.77      0.81        0.73        
test_ngram_3_logistic_regression0.76      0.75      0.79        0.71        
test_ngram_3_naive_bayes      0.74      0.73      0.78        0.68        
test_ngram_2_logistic_regression0.69      0.68      0.73        0.64        
test_ngram_2_naive_bayes      0.68      0.67      0.72        0.61        
test_ngram_1_logistic_regression0.61      0.60      0.64        0.57        
test_ngram_8_random_forest    0.61      0.54      0.72        0.37        
test_ngram_1_naive_bayes      0.60      0.59      0.64        0.55        
test_ngram_1_svm              0.43      0.30      0.00        0.60        
test_ngram_2_svm              0.43      0.30      0.00        0.60        
test_ngram_3_svm              0.43      0.30      0.00        0.60        
test_ngram_4_svm              0.43      0.30      0.00        0.60        
test_ngram_5_svm              0.43      0.30      0.00        0.60        
test_ngram_6_svm              0.43      0.30      0.00        0.60        
test_ngram_7_logistic_regression0.43      0.30      0.00        0.60        
test_ngram_7_naive_bayes      0.43      0.30      0.00        0.60        
test_ngram_7_random_forest    0.43      0.30      0.00        0.60        
test_ngram_7_svm              0.43      0.30      0.00        0.60        
test_ngram_8_logistic_regression0.43      0.31      0.01        0.60        
test_ngram_8_naive_bayes      0.43      0.30      0.00        0.60        
test_ngram_8_svm              0.43      0.30      0.01        0.60        


WORD MODEL RESULTS
--------------------------------------------------------------------------------
Model                         Accuracy  F1-Score  English F1  Pidgin F1   
--------------------------------------------------------------------------------
test_word_random_forest       0.52      0.48      0.34        0.62        
test_word_logistic_regression 0.44      0.32      0.04        0.60        
test_word_naive_bayes         0.43      0.31      0.01        0.60        
test_word_svm                 0.43      0.31      0.01        0.60        


DETAILED MODEL INFORMATION
================================================================================

MODEL: test_ngram_2_random_forest
--------------------------------------------------------------------------------
Accuracy: 0.89
Precision: 0.88
Recall: 0.88
F1-Score: 0.88

English Metrics:
  Precision: 0.90
  Recall: 0.90
  F1-Score: 0.90

Pidgin Metrics:
  Precision: 0.87
  Recall: 0.86
  F1-Score: 0.87

Confusion Matrix:
  TP_English: 27113
  FP_English: 3156
  FN_English: 2858
  TP_Pidgin: 19562

Language Distribution:
  Model: 0.57 English, 0.43 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 46675

================================================================================

MODEL: test_ngram_3_random_forest
--------------------------------------------------------------------------------
Accuracy: 0.89
Precision: 0.89
Recall: 0.89
F1-Score: 0.89

English Metrics:
  Precision: 0.90
  Recall: 0.91
  F1-Score: 0.91

Pidgin Metrics:
  Precision: 0.88
  Recall: 0.87
  F1-Score: 0.87

Confusion Matrix:
  TP_English: 27257
  FP_English: 2986
  FN_English: 2714
  TP_Pidgin: 19732

Language Distribution:
  Model: 0.57 English, 0.43 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 46989

================================================================================

MODEL: test_ngram_4_random_forest
--------------------------------------------------------------------------------
Accuracy: 0.89
Precision: 0.89
Recall: 0.89
F1-Score: 0.89

English Metrics:
  Precision: 0.90
  Recall: 0.91
  F1-Score: 0.90

Pidgin Metrics:
  Precision: 0.88
  Recall: 0.86
  F1-Score: 0.87

Confusion Matrix:
  TP_English: 27395
  FP_English: 3199
  FN_English: 2576
  TP_Pidgin: 19519

Language Distribution:
  Model: 0.58 English, 0.42 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 46914

================================================================================

MODEL: test_ngram_5_random_forest
--------------------------------------------------------------------------------
Accuracy: 0.89
Precision: 0.89
Recall: 0.89
F1-Score: 0.89

English Metrics:
  Precision: 0.89
  Recall: 0.92
  F1-Score: 0.91

Pidgin Metrics:
  Precision: 0.89
  Recall: 0.85
  F1-Score: 0.87

Confusion Matrix:
  TP_English: 27667
  FP_English: 3423
  FN_English: 2304
  TP_Pidgin: 19295

Language Distribution:
  Model: 0.59 English, 0.41 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 46962

================================================================================

MODEL: test_ngram_6_random_forest
--------------------------------------------------------------------------------
Accuracy: 0.89
Precision: 0.89
Recall: 0.88
F1-Score: 0.88

English Metrics:
  Precision: 0.88
  Recall: 0.93
  F1-Score: 0.90

Pidgin Metrics:
  Precision: 0.90
  Recall: 0.83
  F1-Score: 0.86

Confusion Matrix:
  TP_English: 27815
  FP_English: 3805
  FN_English: 2156
  TP_Pidgin: 18913

Language Distribution:
  Model: 0.60 English, 0.40 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 46728

================================================================================

MODEL: test_ngram_6_logistic_regression
--------------------------------------------------------------------------------
Accuracy: 0.87
Precision: 0.87
Recall: 0.87
F1-Score: 0.87

English Metrics:
  Precision: 0.88
  Recall: 0.90
  F1-Score: 0.89

Pidgin Metrics:
  Precision: 0.86
  Recall: 0.84
  F1-Score: 0.85

Confusion Matrix:
  TP_English: 26939
  FP_English: 3578
  FN_English: 3032
  TP_Pidgin: 19140

Language Distribution:
  Model: 0.58 English, 0.42 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 46079

================================================================================

MODEL: test_ngram_6_naive_bayes
--------------------------------------------------------------------------------
Accuracy: 0.85
Precision: 0.85
Recall: 0.85
F1-Score: 0.85

English Metrics:
  Precision: 0.85
  Recall: 0.90
  F1-Score: 0.88

Pidgin Metrics:
  Precision: 0.86
  Recall: 0.80
  F1-Score: 0.82

Confusion Matrix:
  TP_English: 26930
  FP_English: 4643
  FN_English: 3041
  TP_Pidgin: 18075

Language Distribution:
  Model: 0.60 English, 0.40 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 45005

================================================================================

MODEL: test_ngram_5_logistic_regression
--------------------------------------------------------------------------------
Accuracy: 0.84
Precision: 0.84
Recall: 0.84
F1-Score: 0.84

English Metrics:
  Precision: 0.85
  Recall: 0.88
  F1-Score: 0.86

Pidgin Metrics:
  Precision: 0.83
  Recall: 0.80
  F1-Score: 0.82

Confusion Matrix:
  TP_English: 26276
  FP_English: 4518
  FN_English: 3695
  TP_Pidgin: 18200

Language Distribution:
  Model: 0.58 English, 0.42 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 44476

================================================================================

MODEL: test_ngram_5_naive_bayes
--------------------------------------------------------------------------------
Accuracy: 0.82
Precision: 0.82
Recall: 0.81
F1-Score: 0.81

English Metrics:
  Precision: 0.82
  Recall: 0.87
  F1-Score: 0.85

Pidgin Metrics:
  Precision: 0.82
  Recall: 0.75
  F1-Score: 0.78

Confusion Matrix:
  TP_English: 26159
  FP_English: 5689
  FN_English: 3812
  TP_Pidgin: 17029

Language Distribution:
  Model: 0.60 English, 0.40 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 43188

================================================================================

MODEL: test_ngram_4_logistic_regression
--------------------------------------------------------------------------------
Accuracy: 0.80
Precision: 0.80
Recall: 0.79
F1-Score: 0.79

English Metrics:
  Precision: 0.81
  Recall: 0.85
  F1-Score: 0.83

Pidgin Metrics:
  Precision: 0.78
  Recall: 0.74
  F1-Score: 0.76

Confusion Matrix:
  TP_English: 25357
  FP_English: 5905
  FN_English: 4614
  TP_Pidgin: 16813

Language Distribution:
  Model: 0.59 English, 0.41 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 42170

================================================================================

MODEL: test_bilstm
--------------------------------------------------------------------------------
Accuracy: 0.78
Precision: 0.77
Recall: 0.77
F1-Score: 0.77

English Metrics:
  Precision: 0.78
  Recall: 0.84
  F1-Score: 0.81

Pidgin Metrics:
  Precision: 0.76
  Recall: 0.70
  F1-Score: 0.73

Confusion Matrix:
  TP_English: 25056
  FP_English: 6870
  FN_English: 4915
  TP_Pidgin: 15848

Language Distribution:
  Model: 0.61 English, 0.39 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 40904

================================================================================

MODEL: test_ngram_1_random_forest
--------------------------------------------------------------------------------
Accuracy: 0.78
Precision: 0.77
Recall: 0.77
F1-Score: 0.77

English Metrics:
  Precision: 0.82
  Recall: 0.78
  F1-Score: 0.80

Pidgin Metrics:
  Precision: 0.73
  Recall: 0.77
  F1-Score: 0.75

Confusion Matrix:
  TP_English: 23451
  FP_English: 5323
  FN_English: 6520
  TP_Pidgin: 17395

Language Distribution:
  Model: 0.55 English, 0.45 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 40846

================================================================================

MODEL: test_ngram_4_naive_bayes
--------------------------------------------------------------------------------
Accuracy: 0.78
Precision: 0.78
Recall: 0.77
F1-Score: 0.77

English Metrics:
  Precision: 0.79
  Recall: 0.84
  F1-Score: 0.81

Pidgin Metrics:
  Precision: 0.77
  Recall: 0.70
  F1-Score: 0.73

Confusion Matrix:
  TP_English: 25088
  FP_English: 6786
  FN_English: 4883
  TP_Pidgin: 15932

Language Distribution:
  Model: 0.60 English, 0.40 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 41020

================================================================================

MODEL: test_ngram_3_logistic_regression
--------------------------------------------------------------------------------
Accuracy: 0.76
Precision: 0.75
Recall: 0.75
F1-Score: 0.75

English Metrics:
  Precision: 0.77
  Recall: 0.81
  F1-Score: 0.79

Pidgin Metrics:
  Precision: 0.73
  Recall: 0.68
  F1-Score: 0.71

Confusion Matrix:
  TP_English: 24293
  FP_English: 7179
  FN_English: 5678
  TP_Pidgin: 15539

Language Distribution:
  Model: 0.60 English, 0.40 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 39832

================================================================================

MODEL: test_ngram_3_naive_bayes
--------------------------------------------------------------------------------
Accuracy: 0.74
Precision: 0.73
Recall: 0.73
F1-Score: 0.73

English Metrics:
  Precision: 0.75
  Recall: 0.81
  F1-Score: 0.78

Pidgin Metrics:
  Precision: 0.72
  Recall: 0.65
  F1-Score: 0.68

Confusion Matrix:
  TP_English: 24235
  FP_English: 8052
  FN_English: 5736
  TP_Pidgin: 14666

Language Distribution:
  Model: 0.61 English, 0.39 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 38901

================================================================================

MODEL: test_ngram_2_logistic_regression
--------------------------------------------------------------------------------
Accuracy: 0.69
Precision: 0.68
Recall: 0.68
F1-Score: 0.68

English Metrics:
  Precision: 0.72
  Recall: 0.74
  F1-Score: 0.73

Pidgin Metrics:
  Precision: 0.65
  Recall: 0.62
  F1-Score: 0.64

Confusion Matrix:
  TP_English: 22252
  FP_English: 8545
  FN_English: 7719
  TP_Pidgin: 14173

Language Distribution:
  Model: 0.58 English, 0.42 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 36425

================================================================================

MODEL: test_ngram_2_naive_bayes
--------------------------------------------------------------------------------
Accuracy: 0.68
Precision: 0.67
Recall: 0.67
F1-Score: 0.67

English Metrics:
  Precision: 0.71
  Recall: 0.74
  F1-Score: 0.72

Pidgin Metrics:
  Precision: 0.64
  Recall: 0.59
  F1-Score: 0.61

Confusion Matrix:
  TP_English: 22293
  FP_English: 9283
  FN_English: 7678
  TP_Pidgin: 13435

Language Distribution:
  Model: 0.60 English, 0.40 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 35728

================================================================================

MODEL: test_ngram_1_logistic_regression
--------------------------------------------------------------------------------
Accuracy: 0.61
Precision: 0.60
Recall: 0.61
F1-Score: 0.60

English Metrics:
  Precision: 0.67
  Recall: 0.62
  F1-Score: 0.64

Pidgin Metrics:
  Precision: 0.54
  Recall: 0.59
  F1-Score: 0.57

Confusion Matrix:
  TP_English: 18531
  FP_English: 9221
  FN_English: 11440
  TP_Pidgin: 13497

Language Distribution:
  Model: 0.53 English, 0.47 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 32028

================================================================================

MODEL: test_ngram_8_random_forest
--------------------------------------------------------------------------------
Accuracy: 0.61
Precision: 0.61
Recall: 0.57
F1-Score: 0.54

English Metrics:
  Precision: 0.61
  Recall: 0.87
  F1-Score: 0.72

Pidgin Metrics:
  Precision: 0.61
  Recall: 0.27
  F1-Score: 0.37

Confusion Matrix:
  TP_English: 26055
  FP_English: 16653
  FN_English: 3916
  TP_Pidgin: 6065

Language Distribution:
  Model: 0.81 English, 0.19 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 32120

================================================================================

MODEL: test_ngram_1_naive_bayes
--------------------------------------------------------------------------------
Accuracy: 0.60
Precision: 0.59
Recall: 0.59
F1-Score: 0.59

English Metrics:
  Precision: 0.65
  Recall: 0.63
  F1-Score: 0.64

Pidgin Metrics:
  Precision: 0.53
  Recall: 0.56
  F1-Score: 0.55

Confusion Matrix:
  TP_English: 18753
  FP_English: 9984
  FN_English: 11218
  TP_Pidgin: 12734

Language Distribution:
  Model: 0.55 English, 0.45 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 31487

================================================================================

MODEL: test_word_random_forest
--------------------------------------------------------------------------------
Accuracy: 0.52
Precision: 0.62
Recall: 0.57
F1-Score: 0.48

English Metrics:
  Precision: 0.77
  Recall: 0.22
  F1-Score: 0.34

Pidgin Metrics:
  Precision: 0.47
  Recall: 0.92
  F1-Score: 0.62

Confusion Matrix:
  TP_English: 6446
  FP_English: 1897
  FN_English: 23525
  TP_Pidgin: 20821

Language Distribution:
  Model: 0.16 English, 0.84 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 27267

================================================================================

MODEL: test_word_logistic_regression
--------------------------------------------------------------------------------
Accuracy: 0.44
Precision: 0.62
Recall: 0.51
F1-Score: 0.32

English Metrics:
  Precision: 0.80
  Recall: 0.02
  F1-Score: 0.04

Pidgin Metrics:
  Precision: 0.43
  Recall: 0.99
  F1-Score: 0.60

Confusion Matrix:
  TP_English: 689
  FP_English: 177
  FN_English: 29282
  TP_Pidgin: 22541

Language Distribution:
  Model: 0.02 English, 0.98 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 23230

================================================================================

MODEL: test_ngram_1_svm
--------------------------------------------------------------------------------
Accuracy: 0.43
Precision: 0.22
Recall: 0.50
F1-Score: 0.30

English Metrics:
  Precision: 0.00
  Recall: 0.00
  F1-Score: 0.00

Pidgin Metrics:
  Precision: 0.43
  Recall: 1.00
  F1-Score: 0.60

Confusion Matrix:
  TP_English: 0
  FP_English: 0
  FN_English: 29971
  TP_Pidgin: 22718

Language Distribution:
  Model: 0.00 English, 1.00 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 22718

================================================================================

MODEL: test_ngram_2_svm
--------------------------------------------------------------------------------
Accuracy: 0.43
Precision: 0.22
Recall: 0.50
F1-Score: 0.30

English Metrics:
  Precision: 0.00
  Recall: 0.00
  F1-Score: 0.00

Pidgin Metrics:
  Precision: 0.43
  Recall: 1.00
  F1-Score: 0.60

Confusion Matrix:
  TP_English: 0
  FP_English: 0
  FN_English: 29971
  TP_Pidgin: 22718

Language Distribution:
  Model: 0.00 English, 1.00 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 22718

================================================================================

MODEL: test_ngram_3_svm
--------------------------------------------------------------------------------
Accuracy: 0.43
Precision: 0.57
Recall: 0.50
F1-Score: 0.30

English Metrics:
  Precision: 0.71
  Recall: 0.00
  F1-Score: 0.00

Pidgin Metrics:
  Precision: 0.43
  Recall: 1.00
  F1-Score: 0.60

Confusion Matrix:
  TP_English: 15
  FP_English: 6
  FN_English: 29956
  TP_Pidgin: 22712

Language Distribution:
  Model: 0.00 English, 1.00 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 22727

================================================================================

MODEL: test_ngram_4_svm
--------------------------------------------------------------------------------
Accuracy: 0.43
Precision: 0.22
Recall: 0.50
F1-Score: 0.30

English Metrics:
  Precision: 0.00
  Recall: 0.00
  F1-Score: 0.00

Pidgin Metrics:
  Precision: 0.43
  Recall: 1.00
  F1-Score: 0.60

Confusion Matrix:
  TP_English: 0
  FP_English: 0
  FN_English: 29971
  TP_Pidgin: 22718

Language Distribution:
  Model: 0.00 English, 1.00 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 22718

================================================================================

MODEL: test_ngram_5_svm
--------------------------------------------------------------------------------
Accuracy: 0.43
Precision: 0.22
Recall: 0.50
F1-Score: 0.30

English Metrics:
  Precision: 0.00
  Recall: 0.00
  F1-Score: 0.00

Pidgin Metrics:
  Precision: 0.43
  Recall: 1.00
  F1-Score: 0.60

Confusion Matrix:
  TP_English: 0
  FP_English: 0
  FN_English: 29971
  TP_Pidgin: 22718

Language Distribution:
  Model: 0.00 English, 1.00 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 22718

================================================================================

MODEL: test_ngram_6_svm
--------------------------------------------------------------------------------
Accuracy: 0.43
Precision: 0.22
Recall: 0.50
F1-Score: 0.30

English Metrics:
  Precision: 0.00
  Recall: 0.00
  F1-Score: 0.00

Pidgin Metrics:
  Precision: 0.43
  Recall: 1.00
  F1-Score: 0.60

Confusion Matrix:
  TP_English: 0
  FP_English: 0
  FN_English: 29971
  TP_Pidgin: 22718

Language Distribution:
  Model: 0.00 English, 1.00 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 22718

================================================================================

MODEL: test_ngram_7_logistic_regression
--------------------------------------------------------------------------------
Accuracy: 0.43
Precision: 0.22
Recall: 0.50
F1-Score: 0.30

English Metrics:
  Precision: 0.00
  Recall: 0.00
  F1-Score: 0.00

Pidgin Metrics:
  Precision: 0.43
  Recall: 1.00
  F1-Score: 0.60

Confusion Matrix:
  TP_English: 0
  FP_English: 0
  FN_English: 29971
  TP_Pidgin: 22718

Language Distribution:
  Model: 0.00 English, 1.00 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 22718

================================================================================

MODEL: test_ngram_7_naive_bayes
--------------------------------------------------------------------------------
Accuracy: 0.43
Precision: 0.22
Recall: 0.50
F1-Score: 0.30

English Metrics:
  Precision: 0.00
  Recall: 0.00
  F1-Score: 0.00

Pidgin Metrics:
  Precision: 0.43
  Recall: 1.00
  F1-Score: 0.60

Confusion Matrix:
  TP_English: 0
  FP_English: 0
  FN_English: 29971
  TP_Pidgin: 22718

Language Distribution:
  Model: 0.00 English, 1.00 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 22718

================================================================================

MODEL: test_ngram_7_random_forest
--------------------------------------------------------------------------------
Accuracy: 0.43
Precision: 0.22
Recall: 0.50
F1-Score: 0.30

English Metrics:
  Precision: 0.00
  Recall: 0.00
  F1-Score: 0.00

Pidgin Metrics:
  Precision: 0.43
  Recall: 1.00
  F1-Score: 0.60

Confusion Matrix:
  TP_English: 0
  FP_English: 0
  FN_English: 29971
  TP_Pidgin: 22718

Language Distribution:
  Model: 0.00 English, 1.00 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 22718

================================================================================

MODEL: test_ngram_7_svm
--------------------------------------------------------------------------------
Accuracy: 0.43
Precision: 0.22
Recall: 0.50
F1-Score: 0.30

English Metrics:
  Precision: 0.00
  Recall: 0.00
  F1-Score: 0.00

Pidgin Metrics:
  Precision: 0.43
  Recall: 1.00
  F1-Score: 0.60

Confusion Matrix:
  TP_English: 0
  FP_English: 0
  FN_English: 29971
  TP_Pidgin: 22718

Language Distribution:
  Model: 0.00 English, 1.00 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 22718

================================================================================

MODEL: test_ngram_8_logistic_regression
--------------------------------------------------------------------------------
Accuracy: 0.43
Precision: 0.61
Recall: 0.50
F1-Score: 0.31

English Metrics:
  Precision: 0.79
  Recall: 0.01
  F1-Score: 0.01

Pidgin Metrics:
  Precision: 0.43
  Recall: 1.00
  F1-Score: 0.60

Confusion Matrix:
  TP_English: 165
  FP_English: 45
  FN_English: 29806
  TP_Pidgin: 22673

Language Distribution:
  Model: 0.00 English, 1.00 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 22838

================================================================================

MODEL: test_ngram_8_naive_bayes
--------------------------------------------------------------------------------
Accuracy: 0.43
Precision: 0.61
Recall: 0.50
F1-Score: 0.30

English Metrics:
  Precision: 0.79
  Recall: 0.00
  F1-Score: 0.00

Pidgin Metrics:
  Precision: 0.43
  Recall: 1.00
  F1-Score: 0.60

Confusion Matrix:
  TP_English: 31
  FP_English: 8
  FN_English: 29940
  TP_Pidgin: 22710

Language Distribution:
  Model: 0.00 English, 1.00 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 22741

================================================================================

MODEL: test_ngram_8_svm
--------------------------------------------------------------------------------
Accuracy: 0.43
Precision: 0.59
Recall: 0.50
F1-Score: 0.30

English Metrics:
  Precision: 0.75
  Recall: 0.00
  F1-Score: 0.01

Pidgin Metrics:
  Precision: 0.43
  Recall: 1.00
  F1-Score: 0.60

Confusion Matrix:
  TP_English: 101
  FP_English: 34
  FN_English: 29870
  TP_Pidgin: 22684

Language Distribution:
  Model: 0.00 English, 1.00 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 22785

================================================================================

MODEL: test_word_naive_bayes
--------------------------------------------------------------------------------
Accuracy: 0.43
Precision: 0.70
Recall: 0.50
F1-Score: 0.31

English Metrics:
  Precision: 0.97
  Recall: 0.00
  F1-Score: 0.01

Pidgin Metrics:
  Precision: 0.43
  Recall: 1.00
  F1-Score: 0.60

Confusion Matrix:
  TP_English: 148
  FP_English: 4
  FN_English: 29823
  TP_Pidgin: 22714

Language Distribution:
  Model: 0.00 English, 1.00 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 22862

================================================================================

MODEL: test_word_svm
--------------------------------------------------------------------------------
Accuracy: 0.43
Precision: 0.61
Recall: 0.50
F1-Score: 0.31

English Metrics:
  Precision: 0.80
  Recall: 0.00
  F1-Score: 0.01

Pidgin Metrics:
  Precision: 0.43
  Recall: 1.00
  F1-Score: 0.60

Confusion Matrix:
  TP_English: 109
  FP_English: 28
  FN_English: 29862
  TP_Pidgin: 22690

Language Distribution:
  Model: 0.00 English, 1.00 Pidgin
  Ground Truth: 0.57 English, 0.43 Pidgin

Total evaluated characters: 52689
Correct predictions: 22799

================================================================================

